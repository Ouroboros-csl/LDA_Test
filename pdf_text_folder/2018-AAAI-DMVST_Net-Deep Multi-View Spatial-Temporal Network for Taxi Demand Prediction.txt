The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)

Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction
Jintao Ke∗
Huaxiu Yao,∗ Fei Wu
Hong Kong University of Science
Pennsylvania State University
{huaxiuyao, fxw133}@ist.psu.edu
and Technology
jke@connect.ust.hk

Xianfeng Tang
Pennsylvania State University
xianfeng@ist.psu.edu

Yitian Jia, Siyu Lu, Pinghua Gong,
Jieping Ye, Didi Chuxing
{jiayitian, lusiyu, gongpinghua, yejieping}@didichuxing.com

Zhenhui Li
Pennsylvania State University
jessieli@ist.psu.edu

Abstract

Taxi demand prediction is an important building block to en-
abling intelligent transportation systems in a smart city. An
accurate prediction model can help the city pre-allocate re-
sources to meet travel demand and to reduce empty taxis on
streets which waste energy and worsen the trafﬁc conges-
tion. With the increasing popularity of taxi requesting ser-
vices such as Uber and Didi Chuxing (in China), we are
able to collect large-scale taxi demand data continuously.
How to utilize such big data to improve the demand pre-
diction is an interesting and critical real-world problem. Tra-
ditional demand prediction methods mostly rely on time se-
ries forecasting techniques, which fail to model the complex
non-linear spatial and temporal relations. Recent advances
in deep learning have shown superior performance on tra-
ditionally challenging tasks such as image classiﬁcation by
learning the complex features and correlations from large-
scale data. This breakthrough has inspired researchers to ex-
plore deep learning techniques on trafﬁc prediction problems.
However, existing methods on trafﬁc prediction have only
considered spatial relation (e.g., using CNN) or temporal re-
lation (e.g., using LSTM) independently. We propose a Deep
Multi-View Spatial-Temporal Network (DMVST-Net) frame-
work to model both spatial and temporal relations. Specif-
ically, our proposed model consists of three views: tempo-
ral view (modeling correlations between future demand val-
ues with near time points via LSTM), spatial view (modeling
local spatial correlation via local CNN), and semantic view
(modeling correlations among regions sharing similar tem-
poral patterns). Experiments on large-scale real taxi demand
data demonstrate effectiveness of our approach over state-of-
the-art methods.

Introduction
Trafﬁc is the pulse of a city that impacts the daily life of
millions of people. One of the most fundamental questions
for future smart cities is how to build an efﬁcient transporta-
tion system. To address this question, a critical component
is an accurate demand prediction model. The better we can
predict demand on travel, the better we can pre-allocate re-
sources to meet the demand and avoid unnecessary energy

∗The paper was done when these authors were interns in Didi

Chuxing.
Copyright c(cid:2) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

consumption. Currently, with the increasing popularity of
taxi requesting services such as Uber and Didi Chuxing, we
are able to collect massive demand data at an unprecedented
scale. The question of how to utilize big data to better predict
trafﬁc demand has drawn increasing attention in AI research
communities.

In this paper, we study the taxi demand prediction prob-
lem; that problem being how to predict the number of taxi
requests for a region in a future timestamp by using his-
torical taxi requesting data. In literature, there has been
a long line of studies in trafﬁc data prediction, including
trafﬁc volume, taxi pick-ups, and trafﬁc in/out ﬂow vol-
ume. To predict trafﬁc, time series prediction methods have
frequently been used. Representatively, autoregressive in-
tegrated moving average (ARIMA) and its variants have
been widely applied for trafﬁc prediction (Li et al. 2012;
Moreira-Matias et al. 2013; Shekhar and Williams 2008).
Based on the time series prediction method, recent stud-
ies further consider spatial relations (Deng et al. 2016;
Tong et al. 2017) and external context data (e.g., venue,
weather, and events) (Pan, Demiryurek, and Shahabi 2012;
Wu, Wang, and Li 2016). While these studies show that
prediction can be improved by considering various addi-
tional factors, they still fail to capture the complex nonlinear
spatial-temporal correlations.

Recent advances in deep learning have enabled re-
searchers to model the complex nonlinear relationships and
have shown promising results in computer vision and nat-
ural language processing ﬁelds (LeCun, Bengio, and Hin-
ton 2015). This success has inspired several attempts to use
deep learning techniques on trafﬁc prediction problems. Re-
cent studies (Zhang, Zheng, and Qi 2017; Zhang et al. 2016)
propose to treat the trafﬁc in a city as an image and the
trafﬁc volume for a time period as pixel values. Given a
set of historical trafﬁc images, the model predicts the traf-
ﬁc image for the next timestamp. Convolutional neural net-
work (CNN) is applied to model the complex spatial cor-
relation. Yu et al. (2017) proposes to use Long Short Term
Memory networks (LSTM) to predict loop sensor readings.
They show the proposed LSTM model is capable of mod-
eling complex sequential interactions. These pioneering at-
tempts show superior performance compared with previous
methods based on traditional time series prediction methods.
However, none of them consider spatial relation and tempo-

2588

ral sequential relation simultaneously.

In this paper, we harness the power of CNN and LSTM in
a joint model that captures the complex nonlinear relations
of both space and time. However, we cannot simply apply
CNN and LSTM on demand prediction problem. If treating
the demand over an entire city as an image and applying
CNN on this image, we fail to achieve the best result. We
realize including regions with weak correlations to predict a
target region actually hurts the performance. To address this
issue, we propose a novel local CNN method which only
considers spatially nearby regions. This local CNN method
is motivated by the First Law of Geography: “near things
are more related than distant things,” (Tobler 1970) and it is
also supported by observations from real data that demand
patterns are more correlated for spatially close regions.

While local CNN method ﬁlters weakly correlated remote
regions, this fails to consider the case that two locations
could be spatially distant but are similar in their demand pat-
terns (i.e., on the semantic space). For example, residential
areas may have high demands in the morning when people
transit to work, and commercial areas may be have high de-
mands on weekends. We propose to use a graph of regions to
capture this latent semantic, where the edge represents simi-
larity of demand patterns for a pair of regions. Later, regions
are encoded into vectors via a graph embedding method and
such vectors are used as context features in the model. In the
end, a fully connected neural network component is used for
prediction.

Our method is validated via large-scale real-world taxi de-
mand data from Didi Chuxing. The dataset contains taxi de-
mand requests through Didi service in the city of Guangzhou
in China over a two-month span, with about 300,000 re-
quests per day on average. We conducted extensive exper-
iments to compare with state-of-the-art methods and have
demonstrated the superior performance of our proposed
method.

In summary, our contributions are summarized as follow:
• We proposed a uniﬁed multi-view model that jointly con-

siders the spatial, temporal, and semantic relations.

• We proposed a local CNN model that captures local char-

acteristics of regions in relation to their neighbors.

• We constructed a region graph based on the similarity of
demand patterns in order to model the correlated but spa-
tially distant regions. The latent semantics of regions are
learnt through graph embedding.

• We conducted extensive experiments on a large-scale taxi
request dataset from Didi Chuxing. The results show that
our method consistently outperforms the competing base-
lines.

Related Work
Problems of trafﬁc prediction could include predicting any
trafﬁc related data, such as trafﬁc volume (collected from
GPS or loop sensors), taxi pick-ups or drop-offs, trafﬁc ﬂow,
and taxi demand (our problem). The problem formulation
process for these different types of trafﬁc data is the same.
Essentially, the aim is to predict a trafﬁc-related value for a

location at a timestamp. In this section, we will discuss the
related work on trafﬁc prediction problems.

The traditional approach is to use time series prediction
method. Representatively, autoregressive integrated moving
average (ARIMA) and its variants have been widely used
in trafﬁc prediction problem (Shekhar and Williams 2008;
Li et al. 2012; Moreira-Matias et al. 2013).

Recent studies further explore the utilities of external
context data, such as venue types, weather conditions, and
event information (Pan, Demiryurek, and Shahabi 2012;
Wu, Wang, and Li 2016; Wang et al. 2017; Tong et al.
2017). In addition, various techniques have also been in-
troduced to model spatial interactions. For example, Deng
et al. (2016) used matrix factorization on road networks to
capture a correlation among road connected regions for pre-
dicting trafﬁc volume. Several studies (Tong et al. 2017;
Id´e and Sugiyama 2011; Zheng and Ni 2013) also propose to
smooth the prediction differences for nearby locations and
time points via regularization for close space and time de-
pendency. These studies assume trafﬁc in nearby locations
should be similar. However, all of these methods are based
on the time series prediction methods and fail to model the
complex nonlinear relations of the space and time.

Recently, the success of deep learning in the ﬁelds of
computer vision and natural language processing (LeCun,
Bengio, and Hinton 2015; Krizhevsky, Sutskever, and Hin-
ton 2012) motivates researchers to apply deep learning tech-
niques on trafﬁc prediction problems. For instance, Wang
et al. (2017) designed a neural network framework using
context data from multiple sources and predict the gap be-
tween taxi supply and demand. The method uses extensive
features, but does not model the spatial and temporal inter-
actions.

A line of studies applied CNN to capture spatial corre-
lation by treating the entire city’s trafﬁc as images. For ex-
ample, Ma et al. (2017) utilized CNN on images of trafﬁc
speed for the speed prediction problem. Zhang et al. (2016)
and Zhang, Zheng, and Qi (2017) proposed to use residual
CNN on the images of trafﬁc ﬂow. These methods simply
use CNN on the whole city and will use all the regions for
prediction. We observe that utilizing irrelevant regions (e.g.,
remote regions) for prediction of the target region might ac-
tually hurts the performance. In addition, while these meth-
ods do use trafﬁc images of historical timestamps for predic-
tion, but they do not explicitly model the temporal sequential
dependency.

Another line of studies uses LSTM for modeling sequen-
tial dependency. Yu et al. (2017) proposed to apply Long-
short-term memory (LSTM) network and autoencoder to
capture the sequential dependency for predicting the traf-
ﬁc under extreme conditions, particularly for peak-hour and
post-accident scenarios. However, they do not consider the
spatial relation.

In summary,

the biggest difference of our proposed
method compared with literature is that we consider both
spatial relation and temporal sequential relation in a joint
deep learning model.

2589

Preliminaries
In this section, we ﬁrst ﬁx some notations and deﬁne the
taxi demand problem. We follow previous studies (Zhang,
Zheng, and Qi 2017; Wang et al. 2017) and deﬁne the set
of non-overlapping locations L = {l1, l2, ..., li, ..., lN } as
rectangle partitions of a city, and the set of time intervals as
I = {I0, I1, ..., It, ..., IT }. 30 minutes is set as the length
of the time interval. Alternatively, more sophisticated ways
of partitioning can also be used, such as partition space by
road network (Deng et al. 2016) or hexagonal partitioning.
However, this is not the focus of this paper, and our method-
ology can still be applied. Given the set of locations L and
time intervals T , we further deﬁne the following.

Taxi request: A taxi request o is deﬁned as a tuple
(o.t, o.l, o.u), where o.t is the timestamp, o.l represents the
location, and o.u is user identiﬁcation number. The requester
identiﬁcations are used for ﬁltering duplicated and spammer
requests.

Demand: The demand is deﬁned as the number of taxi
= |{o : o.t ∈
requests at one location per time point, i.e., yi
t
It∧o.l ∈ li}|, where |·| denotes the cardinality of the set. For
simplicity, we use the index of time intervals t representing
It, and the index of locations i representing li for rest of the
paper.

Demand prediction problem: The demand prediction
problem aims to predict the demand at time interval t + 1,
given the data until time interval t. In addition to histori-
cal demand data, we can also incorporate context features
such as temporal features, spatial features, meteorological
features (refer to Data Description section for more details).
We denote those context features for a location i and a time
∈ Rr, where r is the number of fea-
point t as a vector ei
t
tures. Therefore, our ﬁnal goal is to predict
t+1 = F(Y L
yi

t−h,...,t, E L

t−h,...,t

)

for i ∈ L, where Y L
t−h,...,t are historical demands and
E L
t−h,...,t are context features for all locations L for time in-
tervals from t − h to t, where t − h denotes the starting time
interval. We deﬁne our prediction function F(·) on all re-
gions and previous time intervals up to t − h to capture the
complex spatial and temporal interaction among them.

Proposed DMVST-Net Framework
In this section, we provide details for our proposed
Deep Multi-View Spatial-Temporal Network (DMVST-Net)
framework, i.e., our prediction function F. Figure 1 shows
the architecture of our proposed method. Our proposed
model has three views: spatial, temporal, and semantic view.

Spatial View: Local CNN
As we mentioned earlier, including regions with weak cor-
relations to predict a target region actually hurts the per-
formance. To address this issue, we propose a local CNN
method which only considers spatially nearby regions. Our
intuition is motivated by the First Law of Geography (Tobler
1970) - “near things are more related than distant things”.

As shown in Figure 1(a), at each time interval t, we treat
one location i with its surrounding neighborhood as one

S × S image (e.g., 7 × 7 image in Figure 1(a)) having one
channel of demand values (with i being at the center of the
image), where the size S controls the spatial granularity. We
use zero padding for location at boundaries of the city. As
a result, we have an image as a tensor (having one channel)
∈ RS×S×1, for each location i and time interval t. The
Yi
t
local CNN takes Yi
and feeds it into K convo-
lutional layers. The transformation at each layer k is deﬁned
as follows:

t as input Yi,0

t

Yi,k
t

= f (Yi,k−1
t

∗ Wk
t

+ bk
t

),

(1)
where ∗ denotes the convolutional operation and f (·) is an
activation function. In this paper, we use the rectiﬁer func-
tion as the activation, i.e., f (z) = max(0, z). Wk
t and bk
t
are two sets of parameters in the kth convolution layer. Note
1,...,K
that the parameters W
are shared across
t
all regions i ∈ L to make the computation tractable.

1,...,K
t

and b

t

After K convolution layers, we use a ﬂatten layer to
∈ RS×S×λ to a feature vector
transform the output Yi,K
∈ RS2λ for region i and time interval t. At last, we use
si
t
a fully connected layer to reduce the dimension of spatial
representations si

t, which is deﬁned as:
+ bf c
t
t are two learnable parameter sets at time
∈

where W f c
interval t. Finally, for each time interval t, we get the ˆsi
t
Rd as the representation for region i.

ˆsi
t
and bf c

= f (W f c

t si
t

(2)

),

t

Temporal View: LSTM
The temporal view models sequential relations in the de-
mand time series. We propose to use Long Short-Term
Memory (LSTM) network as our temporal view component.
LSTM (Hochreiter and Schmidhuber 1997) is a type of neu-
ral network structure, which provides a good way to model
sequential dependencies by recursively applying a transition
function to the hidden state vector of the input. It is pro-
posed to address the problem of classic Recurrent Neural
Network (RNN) for its exploding or vanishing of gradient in
the long sequence training (Hochreiter et al. 2001).

LSTM learns sequential correlations stably by maintain-
ing a memory cell ct in time interval t, which can be re-
garded as an accumulation of previous sequential informa-
tion. In each time interval, LSTM takes an input gi
t, ht−1
and ct−1 in this work, and then all information is accumu-
lated to the memory cell when the input gate ii
t is activated.
In addition, LSTM has a forget gate f i
t . If the forget gate
is activated, the network can forget the previous memory
cell ci
t controls the output of the
memory cell. In this study, we follow the version proposed
in (Graves 2013), which is deﬁned as follows:

t−1. Also, the output gate oi

t−1 + bi),
t−1 + bf ),
t−1 + bo),

+ Uihi
+ Uf hi
+ Uohi

= σ(Wigi
t
= σ(Wf gi
t
= σ(Wogi
t
+ Ughi
= tanh(Wggi
t
◦ θi
t−1 + ii
◦ ci
= f i
t,
t
t
).
◦ tanh(ci
= oi
t
t

t−1 + bg),

(3)

ii
t
f i
t
oi
t
θi
t
ci
t
hi
t

2590

a

b

c

7 × 7 Image

7 × 7 Image

7 × 7 Image

a: Spatial View

(cid:3036),(cid:3012)
(cid:2181)(cid:3047)(cid:2879)(cid:3035)(cid:2878)(cid:2869)

FC

(cid:3036),(cid:2868)
(cid:2181)(cid:3047)(cid:2879)(cid:3035)(cid:2878)(cid:2869)

Conv

(cid:3036),(cid:3012)
(cid:2181)(cid:3036)(cid:3036),(cid:3012)(cid:3012)(cid:3012)(cid:3036) (cid:3012)(cid:3012)
(cid:2181)(cid:3047)(cid:2879)(cid:3035)(cid:2878)(cid:2870)

(cid:3036),(cid:2868)
(cid:2181)(cid:3047)(cid:2879)(cid:3035)(cid:2878)(cid:2870)

Conv
Conv

FC
FC

(cid:3036),(cid:3012)
(cid:2181)(cid:2181)(cid:3036)
(cid:2181)(cid:3047)

(cid:3036),(cid:2868)
(cid:2181)(cid:3047)

Conv
Conv

FCFC
FC

(cid:3036)

(cid:2190)(cid:3047)(cid:2879)(cid:3035)(cid:2878)(cid:2869)

(cid:3036)

(cid:2190)(cid:3047)(cid:2879)(cid:3035)(cid:2878)(cid:2870)

(cid:3036)
(cid:2190)(cid:3047)(cid:2879)(cid:2869)

(cid:3036)
(cid:2187)(cid:3047)(cid:2879)(cid:3035)(cid:2878)(cid:2869)

(cid:3036)
(cid:3548)(cid:2201)(cid:3047)(cid:2879)(cid:3035)(cid:2878)(cid:2869)

(cid:3036)
(cid:2187)(cid:3047)(cid:2879)(cid:3035)(cid:2878)(cid:2869)

(cid:3036)
(cid:3548)(cid:2201)(cid:3047)(cid:2879)(cid:3035)(cid:2878)(cid:2869)

(cid:3036)
(cid:2187)(cid:3047)

(cid:3036)
(cid:3548)(cid:2201)(cid:3047)

40

20

0

40

20

0

1 2 3 4 5 6 7 8 9 10 11 12 13

Embed

(cid:2195)(cid:3036)

FC

1 2 3 4 5 6 7 8 9 10 11 12 13

b: Temporal View

c: Semantic View

(cid:3036)
(cid:1877)(cid:3047)(cid:2878)(cid:2869)

Loss

(cid:3036)
(cid:3548)(cid:1877)(cid:3047)(cid:2878)(cid:2869)

FC

(cid:3036)
(cid:2190)(cid:3047)

(cid:3549)(cid:2195)(cid:3036)

Figure 1: The Architecture of DMVST-Net. (a). The spatial component uses a local CNN to capture spatial dependency among
nearby regions. The local CNN includes several convolutional layers. A fully connected layer is used at the end to get a low
dimensional representation. (b). The temporal view employs a LSTM model, which takes the representations from the spatial
view and concatenates them with context features at corresponding times. (c). The semantic view ﬁrst constructs a weighted
graph of regions (with weights representing functional similarity). Nodes are encoded into vectors. A fully connected layer is
used at the end for jointly training. Finally, a fully connected neural network is used for prediction.

where ◦ denotes Hadamard product and tanh is hyper-
bolic tangent function. Both functions are element-wise.
Wa, Ua, ba (a ∈ {i, f, o, g}) are all learnable parameters.
The number of time intervals in LSTM is h and the output
of region i of LSTM after h time intervals is hi
t.

As Figure 1(b) shows, the temporal component takes rep-
resentations from the spatial view and concatenates them
with context features. More speciﬁcally, we deﬁne:
gi
t

= ˆsi
t
where ⊕ denotes the concatenation operator, therefore, gi
t
Rr+d.

⊕ ei
t,

(4)
∈

Semantic View: Structural Embedding
Intuitively, locations sharing similar functionality may have
similar demand patterns, e.g., residential areas may have a
high number of demands in the morning when people transit
to work, and commercial areas may expect to have high de-
mands on weekends. Similar regions may not necessarily be
close in space. Therefore, we construct a graph of locations
representing functional (semantic) similarity among regions.
We deﬁne the semantic graph of location as G =
(V, E, D), where the set of locations L are nodes V = L,
E ∈ V × V is the edge set, and D is a set of similarity on all
the edges. We use Dynamic Time Warping (DTW) to mea-
sure the similarity ωij between node (location) i and node
(location) j.

ωij = exp(−αDTW(i, j)),
(5)
where α is the parameter that controls the decay rate of the
distance (in this paper, α = 1), and DTW(i, j) is the dy-
namic time warping distance between the demand patterns

2591

of two locations. We use the average weekly demand time
series as the demand patterns. The average is computed on
the training data in the experiment. The graph is fully con-
nected because every two regions can be reached.

In order to encode each node into a low dimensional vec-
tor and maintain the structural information, we apply a graph
embedding method on the graph. For each node i (location),
the embedding method outputs the embedded feature vector
mi. In addition, in order to co-train the embedded mi with
our whole network architecture, we feed the feature vector
mi to a fully connected layer, which is deﬁned as:

ˆmi = f (Wf emi + bf e),

(6)

Wf e and bf e are both learnable parameters. In this paper, we
use LINE for generating embeddings (Tang et al. 2015).

Prediction Component
Recall that our goal is to predict the demand at t + 1 given
the data till t. We join three views together by concatenating
ˆmi with the output hi

t of LSTM:
= hi
qi
t
t

⊕ ˆmi.

(7)

Note that the output of LSTM hi
temporal and spatial view. Then we feed qi
nected network to get the ﬁnal prediction value ˆyi
region. We deﬁne our ﬁnal prediction function as:
t+1 = σ(Wf f qi
ˆyi
t

t contains both effects of
t to the fully con-
t+1 for each

(8)
where Wf f and bf f are learnable parameters. σ(x) is a Sig-
moid function deﬁned as σ(x) = 1/(1+e−x). The output of

+ bf f ),

Algorithm 1: Training Pipeline of DMVST-Net
Input: Historical observations: Y L
1,...,t; Context

features: E L
G = (V, E, D); Length of the time period h;

t−h,...,t; Region structure graph

Output: Learned DMVST-Net model

1 Initialization;
2 for ∀i ∈ L do
3

Use LINE on G and get the embedding result mi;
for ∀t ∈ [h, T ] do
Sspa = [Yi
t−h+1, Yi
t−h+2, ..., Yi
t
];
Scox = [ei
t−h+2, ..., ei
t−h+1, ei
t
t+1 > to Ωbt ;
Append < {Sspa, Scox, mi}, yi

];

4

5

6

7

end

8
9 end
10 Initialize all learnable parameters θ in DMVST-Net;
11 repeat
12

Randomly select a batch of instance Ωbt from Ω;
Optimize θ by minimizing the loss function
Eq. (9) with Ωbt
14 until stopping criteria is met;

13

our model is in [0, 1], as the demand values are normalized.
We later denormalize the prediction to get the actual demand
values.

Loss function
In this section, we provide details about the loss function
used for jointly training our proposed model. The loss func-
tion we used is deﬁned as:

L(θ) =

N(cid:2)

i=1

((yi

t+1 − ˆyi

t+1)2 + γ(

t+1 − ˆyi
yi
yi
t+1

t+1

)2),

(9)

where θ are all learnable parameters in the DMVST-Net and
γ is a hyper parameter. The loss function consists of two
parts: mean square loss and square of mean absolute per-
centage loss. In practice, mean square error is more rele-
vant to predictions of large values. To avoid the training
being dominated by large value samples, we in addition
minimize the mean absolute percentage loss. Note that, in
the experiment, all compared regression methods use the
same loss function as deﬁned in Eq. (9) for fair compari-
son. The training pipeline is outlined in Algorithm 1. We use
Adam (Kingma and Ba 2014) for optimization. We use Ten-
sorﬂow and Keras (Chollet and others 2015) to implement
our proposed model.

Experiment

Dataset Description
In this paper, we use a large-scale online taxi request dataset
collected from Didi Chuxing, which is one of the largest on-
line car-hailing companies in China. The dataset contains
taxi requests from 02/01/2017 to 03/26/2017 for the city
of Guangzhou. There are 20 × 20 regions in our data. The
size of each region is 0.7km × 0.7km. There are about

2592

ξ(cid:2)

1

ξ
(cid:3)
(cid:4)
(cid:4)
(cid:5) 1
ξ

i=1

ξ(cid:2)

i=1

300, 000 requests each day on average. The context features
used in our experiment are the similar types of features used
in (Tong et al. 2017). These features include temporal fea-
tures (e.g., the average demand value in the last four time
intervals), spatial features (e.g., longitude and latitude of the
region center), meteorological features (e.g., weather condi-
tion), event features (e.g., holiday).

In the experiment,

the data from 02/01/2017 to
03/19/2017 is used for training (47 days), and the data from
03/20/2017 to 03/26/2017 (7 days) is used for testing. We
use half an hour as the length of the time interval. When
testing the prediction result, we use the previous 8 time in-
tervals (i.e., 4 hours) to predict the taxi demand in the next
time interval. In our experiment, we ﬁlter the samples with
demand values less than 10. This is a common practice used
in industry. Because in the real-world applications, people
do not care about such low-demand scenarios.

Evaluation Metric
We use Mean Average Percentage Error (MAPE) and
Rooted Mean Square Error (RMSE) to evaluate our algo-
rithm, which are deﬁned as follows:

M AP E =

t+1|

|ˆyi

t+1 − yi
yi
t+1

,

(10)

RM SE =

(ˆyi

t+1 − yi

t+1)2,

(11)

t+1 and yi

where ˆyi
t+1 mean the prediction value and real
value of region i for time interval t + 1, and where ξ is total
number of samples.

Methods for Comparison
We compared our model with the following methods, and
tuned the parameters for all methods. We then reported the
best performance.
• Historical average (HA): Historical average predicts the
demand using average values of previous demands at the
location given in the same relative time interval (i.e., the
same time of the day).

• Autoregressive integrated moving average (ARIMA):
ARIMA is a well-known model for forecasting time se-
ries which combines moving average and autoregressive
components for modeling time series.

• Linear regression (LR): We compare our method with
different versions of linear regression methods: ordinary
least squares regression (OLSR), Ridge Regression (i.e.,
with (cid:8)2-norm regularization), and Lasso (i.e., with (cid:8)1-
norm regularization).

• Multiple layer perceptron (MLP): We compare our
method with a neural network of four fully connected lay-
ers. The number of hidden units are 128, 128, 64, and 64
respectively.

• XGBoost (Chen and Guestrin 2016): XGBoost is a pow-
erful boosting tree based method and is widely used in
data mining applications.

• ST-ResNet (Zhang, Zheng, and Qi 2017): ST-ResNet is
a deep learning based approach for trafﬁc prediction. The
method constructs a city’s trafﬁc density map at different
times as images. CNN is used to extract features from his-
torical images.

We used the same context features for all regression methods
above. For fair comparisons, all methods (except ARIMA
and HA) use the same loss function as our method deﬁned
in Eq. (9).

We also studied the effect of different view components

proposed in our method.
• Temporal view: For this variant, we used only LSTM
with inputs as context features. Note that, if we do not
use any context features but only use the demand value
of last timestamp as input, LSTM does not perform well.
It is necessary to use context features to enable LSTM to
model the complex sequential interactions for these fea-
tures.

• Temporal view + Semantic view: This method captures
both temporal dependency and semantic information.
• Temporal view + Spatial (Neighbors) view: In this vari-
ant, we used the demand values of nearby regions at time
interval t as ˆsi
t and combined them with context features
as the input of LSTM. We wanted to demonstrate that sim-
ply using neighboring regions as features cannot model
the complex spatial relations as our proposed local CNN
method.

• Temporal view + Spatial (LCNN) view: This variant
considers both temporal and local spatial views. The spa-
tial view uses the proposed local CNN for considering
neighboring relation. Note that when our local CNN uses
a local window that is large enough to cover the whole
city, it is the same as the global CNN method. We stud-
ied the performance of different parameters and show that
if the size is too large, the performance is worse, which
indicates the importance of locality.

• DMVST-Net: Our proposed model, which combines spa-

tial, temporal and semantic views.

Preprocessing and Parameters
We normalized the demand values for all locations to [0, 1]
by using Max-Min normalization on the training set. We
used one-hot encoding to transform discrete features (e.g.,
holidays and weather conditions) and used Max-Min nor-
malization to scale the continuous features (e.g., the average
of demand value in last four time intervals). As our method
outputs a value in [0, 1], we applied the inverse of the Max-
Min transformation obtained on training set to recover the
demand value.

All these experiments were run on a cluster with four
NVIDIA P100 GPUs. The size of each neighborhood con-
sidered was set as 9 × 9 (i.e., S = 9), which corresponds
to 6km × 6km rectangles. For spatial view, we set K = 3
(number of layers), τ = 3×3 (size of ﬁlter), λ = 64 (number
of ﬁlters used), and d = 64 (dimension of the output). For
the temporal component, we set the sequence length h = 8

Table 1: Comparison with Different Baselines

Method
Historical average
ARIMA
Ordinary least square regression
Ridge regression
Lasso
Multiple layer perceptron
XGBoost
ST-ResNet
DMVST-Net

MAPE RMSE
12.167
0.2513
11.932
0.2215
10.234
0.2063
10.224
0.2061
10.327
0.2091
10.609
0.1840
10.012
0.1953
10.298
0.1971
9.642
0.1616

(i.e., 4 hours) for LSTM. The output dimension of graph em-
bedding is set as 32. The output dimension for the semantic
view is set to 6. We used Sigmoid function as the activation
function for the fully connected layer in the ﬁnal prediction
component. Activation functions in other fully connected
layers are ReLU. Batch normalization is used in the local
CNN component. The batch size in our experiment was set
to 64. The ﬁrst 90% of the training samples were selected for
training each model and the remaining 10% were in the val-
idation set for parameter tuning. We also used early-stop in
all the experiments. The early-stop round and the max epoch
were set to 10 and 100 in the experiment, respectively.

Performance Comparison
Comparison with state-of-the-art methods. Table 1
shows the performance of the proposed method as com-
pared to all other competing methods. DMVST-Net achieves
the lowest MAPE (0.1616) and the lowest RMSE (9.642)
among all the methods, which is 12.17% (MAPE) and
3.70% (RMSE) relative improvement over the best perfor-
mance among baseline methods. More speciﬁcally, we can
see that HA and ARIMA perform poorly (i.e., have a MAPE
of 0.2513 and 0.2215, respectively), as they rely purely on
historical demand values for prediction. Regression methods
(OLSR, LASSO, Ridge, MLP and XGBoost) further con-
sider context features and therefore achieve better perfor-
mance. Note that the regression methods use the same loss
function as our method deﬁned in Eq. (9). However, the re-
gression methods do not model the temporal and spatial de-
pendency. Consequently, our proposed method signiﬁcantly
outperforms those methods.

Furthermore, our proposed method achieves 18.01%
(MAPE) and 6.37% relative improvement over ST-ResNet.
Compared with ST-ResNet, our proposed method further
utilizes LSTM to model the temporal dependency, while at
the same time considering context features. In addition, our
use of local CNN and semantic view better captures the cor-
relation among regions.

Comparison with variants of our proposed method. Ta-
ble 2 shows the performance of DMVST-Net and its vari-
ants. First, we can see that both Temporal view + Spatial
(Neighbor) view and Temporal view + Spatial (LCNN) view
achieve a lower MAPE (a reduction of 0.63% and 6.10%,
respectively). The result demonstrates the effectiveness of

2593

Table 2: Comparison with Variants of DMVST-Net

MAPE RMSE
Method
9.812
0.1721
Temporal view
9.789
0.1708
Temporal + Semantic view
9.796
Temporal + Spatial (Neighbor) view 0.1710
9.695
0.1640
Temporal + Spatial (LCNN) view
9.642
0.1616
DMVST-Net

considering neighboring spatial dependency. Furthermore,
Temporal view + Spatial (LCNN) view outperforms Tempo-
ral view + Spatial (Neighbor) view signiﬁcantly, as the local
CNN can better capture the nonlinear relations. On the other
hand, Temporal view + Semantic view has a lower MAPE of
0.1708 and an RMSE of 9.789 compared to Temporal view
only, demonstrating the effectiveness of our semantic view.
Lastly, the performance is best when all views are combined.

Performance on Different Days
Figure 2 shows the performance of different methods on dif-
ferent days of the week. Due to the space limitation, We only
show MAPE here. We get the same conclusions of RMSE.
We exclude the results of HA and ARIMA, as they perform
poorly. We show Ridge regression results as they perform
best among linear regression models. In the ﬁgure, it shows
that our proposed method DMVST-Net outperforms other
methods consistently in all seven days. The result demon-
strates that our method is robust.

Moreover, we can see that predictions on weekends are
generally worse than on weekdays. Since the average num-
ber of demand requests is similar (45.42 and 43.76 for week-
days and weekends, respectively), we believe the prediction
task is harder for weekends as demand patterns are less reg-
ular. For example, we can expect that residential areas may
have high demands in the morning hours on weekdays, as
people need to transit to work. Such regular patterns are less
likely to happen on weekends. To evaluate the robustness of
our method, we look at the relative increase in prediction er-
ror on weekends as compared to weekdays, i.e., deﬁned as
| ¯wk − ¯wd|/ ¯wd, where ¯wd and ¯wk are the average prediction
error of weekdays and weekends, respectively. The results
are shown in Table 3. For our proposed method, the relative
increase in error is the smallest, at 4.04%.

At

the same time, considering temporal view, only
(LSTM) has a relative increase in error of 4.77%, while the
increase is more than 10% for Ridge regression, MLP, and
XGBoost. The more stable performance of LSTM can be at-
tributed to its modeling of the temporal dependency. We see
that ST-ResNet has a more consistent performance (relative
increase in error of 4.41%), as the method further models the
spatial dependency. Finally, our proposed method is more
robust than ST-ResNet.

Inﬂuence of Sequence Length for LSTM
In this section, we study how the sequence length for LSTM
affects the performance. Figure 3a shows the prediction er-

Table 3: Relative Increase in Error (RIE) on Weekends to
Weekdays

Method RIDGE MLP XGBoost ST-ResNet Temporal DMVST-Net

RIE

14.91% 10.71% 16.08%

4.41%

4.77%

4.04%

E
P
A
M

0.24

0.22

0.2

0.18

0.16

0.14

Mon

Tue

Wed

Thu

Fri

Sat

Sun

Ridge

MLP

Xgboost

ST-ResNet

Temporal

DMVST-Net

Figure 2: The Results of Different Days.

ror of MAPE with respect to the length. We can see that
when the length is 4 hours, our method achieves the best
performance. The decreasing trend in MAPE as the length
increases shows the importance of considering the temporal
dependency. Furthermore, as the length increases to more
than 4 hours, the performance slightly degrades but mainly
remains stable. One potential reason is that when consider-
ing longer temporal dependency, more parameters need to
be learned. As a result, the training becomes harder.

Inﬂuence of Input Size for Local CNN
Our intuition was that applying CNN locally avoids learn-
ing relation among weakly related locations. We veriﬁed
that intuition by varying the input size S for local CNN.
As the input size S becomes larger, the model may ﬁt for
relations in a larger area. In Figure 3b, we show the per-
formance of our method with respect to the size of the sur-
rounding neighborhood map. We can see that when there
are three convolutional layers and the size of map is 9 × 9,
the method achieves the best performance. The prediction
error increases as the size decreases to 5 × 5. This may be
due to the fact that locally correlated neighboring locations
are not fully covered. Furthermore, the prediction error in-
creases signiﬁcantly (more than 3.46%), as the size increases
to 13 × 13 (where each area approximately covers more than
40% of the space in GuangZhou). The result suggests that
locally signiﬁcant correlations may be averaged as the size
increases. We also increased the number of convolution lay-
ers to four and ﬁve layers, as the CNN needed to cover larger
area. However, we observed similar trends of prediction er-
ror, as shown in Figure 3b. We can now see that the input
size for local CNN when the method performs best remains
consistent (i.e., the size of map is 9 × 9).

Conclusion and Discussion
The purpose of this paper is to inform of our proposal
of a novel Deep Multi-View Spatial-Temporal Network
(DMVST-Net) for predicting taxi demand. Our approach in-

2594

E
P
A
M

0.19

0.18

0.17

0.16

0.15

3 Layers
4 Layers
5 Layers

0.17

0.165

E
P
A
M

0.16

1 Hour

4 Hours
Sequence Length of LSTM

7 Hours

5 5

11 11 13 13
9 9
Size of Surrounding Neighborhood Map

7 7

(a)

(b)

Figure 3: (a) MAPE with respect to sequence length for
LSTM. (b) MAPE with respect to the input size for local
CNN.

tegrates the spatial, temporal, and semantic views, which are
modeled by local CNN, LSTM and semantic graph embed-
ding, respectively. We evaluated our model on a large-scale
taxi demand dataset. The experiment results show that our
proposed method signiﬁcantly outperforms several compet-
ing methods.

As deep learning methods are often difﬁcult to interpret, it
is important to understand what contributes to the improve-
ment. This is particularly important for policy makers. For
future work, we plan to further investigate the performance
improvement of our approach for better interpretability. In
addition, seeing as the semantic information is implicitly
modeled in this paper, we plan to incorporate more explicit
information (e.g., POI information) in our future work.

Acknowledgments
The work was supported in part by NSF awards #1544455,
#1652525, #1618448, and #1639150. The views and con-
clusions contained in this paper are those of the authors and
should not be interpreted as representing any funding agen-
cies.

References
Chen, T., and Guestrin, C. 2016. Xgboost: A scalable tree boosting
system. In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, 785–794.
ACM.
Chollet, F., et al. 2015. Keras. https://github.com/fchollet/keras.
Deng, D.; Shahabi, C.; Demiryurek, U.; Zhu, L.; Yu, R.; and Liu, Y.
2016. Latent space model for road networks to predict time-varying
trafﬁc. Proceedings of the 22th ACM SIGKDD international con-
ference on Knowledge discovery and data mining.
Graves, A. 2013. Generating sequences with recurrent neural net-
works. arXiv preprint arXiv:1308.0850.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term mem-
ory. Neural computation 9(8):1735–1780.
Hochreiter, S.; Bengio, Y.; Frasconi, P.; and Schmidhuber, J. 2001.
Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term
dependencies.
Id´e, T., and Sugiyama, M. 2011. Trajectory regression on road
networks. In Proceedings of the Twenty-Fifth AAAI Conference on
Artiﬁcial Intelligence, 203–208. AAAI Press.
Kingma, D., and Ba, J. 2014. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980.

2595

Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet
classiﬁcation with deep convolutional neural networks.
In Ad-
vances in neural information processing systems, 1097–1105.
LeCun, Y.; Bengio, Y.; and Hinton, G. 2015. Deep learning. Nature
521(7553):436–444.
Li, X.; Pan, G.; Wu, Z.; Qi, G.; Li, S.; Zhang, D.; Zhang, W.; and
Wang, Z. 2012. Prediction of urban human mobility using large-
scale taxi traces and its applications. Frontiers of Computer Science
6(1):111–121.
Ma, X.; Dai, Z.; He, Z.; Ma, J.; Wang, Y.; and Wang, Y. 2017.
Learning trafﬁc as images: a deep convolutional neural network
for large-scale transportation network speed prediction. Sensors
17(4):818.
Moreira-Matias, L.; Gama, J.; Ferreira, M.; Mendes-Moreira, J.;
and Damas, L. 2013. Predicting taxi–passenger demand using
streaming data. IEEE Transactions on Intelligent Transportation
Systems 14(3):1393–1402.
Pan, B.; Demiryurek, U.; and Shahabi, C. 2012. Utilizing real-
world transportation data for accurate trafﬁc prediction. In Data
Mining (ICDM), 2012 IEEE 12th International Conference on,
595–604. IEEE.
Shekhar, S., and Williams, B. 2008. Adaptive seasonal time se-
ries models for forecasting short-term trafﬁc ﬂow. Transportation
Research Record: Journal of the Transportation Research Board
(2024):116–125.
Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015.
Line: Large-scale information network embedding. In Proceedings
of the 24th International Conference on World Wide Web, 1067–
1077. International World Wide Web Conferences Steering Com-
mittee.
Tobler, W. R. 1970. A computer movie simulating urban growth in
the detroit region. Economic geography 46(sup1):234–240.
Tong, Y.; Chen, Y.; Zhou, Z.; Chen, L.; Wang, J.; Yang, Q.; and
Ye, J. 2017. The simpler the better: A uniﬁed approach to pre-
dicting original taxi demands on large-scale online platforms. In
Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM.
Wang, D.; Cao, W.; Li, J.; and Ye, J. 2017. Deepsd: Supply-
demand prediction for online car-hailing services using deep neural
networks. In Data Engineering (ICDE), 2017 IEEE 33rd Interna-
tional Conference on, 243–254. IEEE.
Wu, F.; Wang, H.; and Li, Z. 2016.
Interpreting trafﬁc dynam-
ics using ubiquitous urban data. In Proceedings of the 24th ACM
SIGSPATIAL International Conference on Advances in Geographic
Information Systems, 69. ACM.
Yu, R.; Li, Y.; Demiryurek, U.; Shahabi, C.; and Liu, Y. 2017.
Deep learning: A generic approach for extreme condition trafﬁc
forecasting. In Proceedings of SIAM International Conference on
Data Mining.
Zhang, J.; Zheng, Y.; Qi, D.; Li, R.; and Yi, X. 2016. Dnn-based
prediction model for spatio-temporal data. In Proceedings of the
24th ACM SIGSPATIAL International Conference on Advances in
Geographic Information Systems, 92. ACM.
Zhang, J.; Zheng, Y.; and Qi, D. 2017. Deep spatio-temporal resid-
ual networks for citywide crowd ﬂows prediction. Proceedings of
the Thirty-First AAAI Conference on Artiﬁcial Intelligence.
Zheng, J., and Ni, L. M. 2013. Time-dependent trajectory re-
gression on road networks via multi-task learning. In Proceedings
of the Twenty-Seventh AAAI Conference on Artiﬁcial Intelligence,
1048–1055. AAAI Press.

